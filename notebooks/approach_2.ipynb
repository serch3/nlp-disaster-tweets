{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with Disaster Tweets\n",
    "\n",
    "### Approach: Transformers - trainer class with pre-trained model DistilRoBERTa-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Analysis\n",
    "- **Dataset size:** \n",
    "    - Training: 7503 samples\n",
    "    - Testing: 3263 samples\n",
    "- **Problem Nature:**\n",
    "    - Tweets are short (max 280 characters).\n",
    "    - Tweets often include noisy data (hashtags, misspellings, slang, abbreviations).\n",
    "    - Context is crucial as tweets might be sarcastic or humorous.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27ac580f9b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/get-started/locally/ | torch\n",
    "# pip install transformers pandas scikit-learn ftfy\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "from ftfy import fix_text\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import warnings\n",
    "\n",
    "# Suppress for a cleaner console\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "# local debugging for nvidia gpu\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleaning pipeline helper.\n",
    "\n",
    "    Notes:\n",
    "        - DistilRoBERTa-base can tokenize and learn from hashtags, mentions, emojis, so we should keep them.\n",
    "        - Replacing links with a placeholder [URL] signals the model a URL was present, possibly useful for context.\n",
    "        - Repeated punctuation could indicate emotion \n",
    "\n",
    "    \"\"\"\n",
    "    # Try to fix broken unicode errors found in dataset (e.g. \"Û÷Institute\" and \"PeaceÛª\")\n",
    "    text = fix_text(text)\n",
    "\n",
    "    # replace links with [URL] placeholder\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' [URL] ', text)  \n",
    "\n",
    "    # Decode common HTML entities (like &amp;)\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Replace multiple spaces/tabs/newlines with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "\n",
    "def preprocess_data(df):\n",
    "    # handle cases where keyword and location are blank\n",
    "    df['keyword'].fillna('', inplace=True)\n",
    "    df['location'].fillna('', inplace=True)\n",
    "\n",
    "    # combine text, keyword, and location into a single column for better context\n",
    "    df['combined_text'] = df[['text', 'keyword', 'location']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "    # apply clean_text function to the combined text column\n",
    "    df['combined_text'] = df['combined_text'].apply(clean_text)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return accuracy and F1 score for predictions.\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids # True labels\n",
    "    preds = np.argmax(pred.predictions, axis=1) # Predicted labels\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "# https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for tweets:\n",
    "        - Tokenizes the text\n",
    "        - Handles cases with or without labels (for training vs. testing)\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text and convert to tensors\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "    \n",
    "        # add labels if they exist (for training)\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: @Daorcey @nsit_ YOUR a great pair. Like a couple of Graywardens fighting the blight...\n",
      "Cleaned:  @Daorcey @nsit_ YOUR a great pair. Like a couple of Graywardens fighting the blight...\n",
      "--------------------------------------------------------------------------------\n",
      "Original: @ArianaGrande I literally walked out of the concert and screamed MY SOUL HAS BEEN BLESSED\n",
      "Cleaned:  @ArianaGrande I literally walked out of the concert and screamed MY SOUL HAS BEEN BLESSED\n",
      "--------------------------------------------------------------------------------\n",
      "Original: MEN CRUSH EVERY FUCKING DAY???????????????????????????? http://t.co/Fs4y1c9mNf\n",
      "Cleaned:  MEN CRUSH EVERY FUCKING DAY???????????????????????????? [URL]\n",
      "--------------------------------------------------------------------------------\n",
      "Original: ÛÏ@based_georgie: yo forreal we need to have like an emergency action plan incase donald trump becomes presidentÛ\n",
      "Whipe that lil baby\n",
      "Cleaned:  ‰ÛÏ@based_georgie: yo forreal we need to have like an emergency action plan incase donald trump becomes president‰Û Whipe that lil baby\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Preview of cleaned tweets from the training dataset\n",
    "sample_tweets = train_df.sample(4)\n",
    "for _, tweet in sample_tweets.iterrows():\n",
    "    original = tweet['text']\n",
    "    cleaned = clean_text(original)\n",
    "    \n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1524' max='1524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1524/1524 01:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.428163</td>\n",
       "      <td>0.828628</td>\n",
       "      <td>0.797203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.464606</td>\n",
       "      <td>0.839790</td>\n",
       "      <td>0.807571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.562177</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.795367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>0.620817</td>\n",
       "      <td>0.831911</td>\n",
       "      <td>0.795200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: {'eval_loss': 0.46460622549057007, 'eval_accuracy': 0.8397898883782009, 'eval_f1': 0.807570977917981, 'eval_runtime': 1.2577, 'eval_samples_per_second': 1210.959, 'eval_steps_per_second': 76.331, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['combined_text'].tolist(), \n",
    "    train_df['target'].tolist(), \n",
    "    test_size=0.2, # 20% validation, 80% training split\n",
    "    random_state=404\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"distilroberta-base\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # Load the tokenizer class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Load modal for sequence classification (binary)\n",
    "\n",
    "# Create training and validation datasets using custom Dataset class\n",
    "train_dataset = TweetDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TweetDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# help(TrainingArguments)\n",
    "\n",
    "# Arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../results',              # Directory to save checkpoints and model predictions\n",
    "    num_train_epochs=4,                  # number of passes over the traning dataset (can overfit quickly since dataset is very small)\n",
    "    per_device_train_batch_size=16,      # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,       # Batch size for evaluation\n",
    "    eval_strategy=\"epoch\",         # Evaluate at the end of each pass\n",
    "    weight_decay=0.01,                   # helps generalization\n",
    "    logging_dir='../logs',\n",
    "    logging_steps=10,                    # Log rate for training info\n",
    "    seed=35,\n",
    "    load_best_model_at_end=True,         # will keep best checkpoint automatically\n",
    "    metric_for_best_model=\"f1\",          # Use F1 score to determine the best model\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer (AdamW optimizer is used by default)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# fine-tune it on the training data\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"../saved_model\")\n",
    "tokenizer.save_pretrained(\"../saved_model\")\n",
    "\n",
    "# Evaluate the model on the validation set and print results\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"Validation results:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the test set (no labels) into a acceptable format for the Trainer to predict on\n",
    "test_dataset = TweetDataset(test_df['combined_text'].tolist(), labels=None, tokenizer=tokenizer)\n",
    "\n",
    "# inference/prediction\n",
    "test_outputs = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract the predicted class (0 or 1)\n",
    "test_predictions = np.argmax(test_outputs.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved as ../results/ap_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a submission format\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"target\": test_predictions\n",
    "})\n",
    "\n",
    "# Save as a CSV file\n",
    "file_name = \"../results/ap_2.csv\" \n",
    "submission.to_csv(file_name, index=False)\n",
    "print(f\"Submission saved as {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
